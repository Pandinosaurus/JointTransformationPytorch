{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint transformations in Pytorch\n",
    "Before feeding a neural network, you may want to perfom a sort of data augmentation through random transformations. In problems like semantic segmentation, you would like to be sure that all the data corresponding to a given image instance are read and transformed the same way (e.g., you want to random crop the radiometric image and the ground truth at the same location). However, random transformations as defined in Pytorch will be random per image. As a result, there is a high chance that an image and its ground truth won't be crop at the same location when using a random crop (or any other random operation). Note that this phenomenon is also true for more than 2 images (e.g., you have optical and multispectral data as well as a ground truth segmentation but you did not stack them for any possible reason - yet all of them should be modified the same way).\n",
    "\n",
    "A very neat approach to solve this problem is to merge (concatenate) all the images corresponding to a data point (ground truth, image, segmentation, etc.) in the channels dimension, then to apply the transform on the whole stack before to split the data again. \n",
    "\n",
    "Following code is based https://github.com/pytorch/vision/issues/9 . It proposes to merge all the images in the channel dimension before applying the transform on the whole stack of images. The major limitation of this approach is that we need to redefine the transformation used in the transforms module of Pytorch. Indeed, Pytorch is based on pillow images (PIL), but PIL does not accept more than 4 channels per image and thus can't transform images with more than 4 channels. As a result, default transformations cannot be used with a huge stack of images: We need to rewrite them with Numpy+OpenCV (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io\n",
    "import math\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCompose(object):\n",
    "    \"\"\"Composes several transforms together, support separate transformations for multiple input.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            if isinstance(t, collections.Sequence):\n",
    "                assert isinstance(img, collections.Sequence) and len(img) == len(t), \"size of image group and transform group does not fit\"\n",
    "                tmp_ = []\n",
    "                for i, im_ in enumerate(img):\n",
    "                    if callable(t[i]):\n",
    "                        tmp_.append(t[i](im_))\n",
    "                    else:\n",
    "                        tmp_.append(im_)\n",
    "                img = tmp_\n",
    "            elif callable(t):\n",
    "                img = t(img)\n",
    "            elif t is None:\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception('unexpected type')                \n",
    "        return img\n",
    "\n",
    "class Merge(object):\n",
    "    \"\"\"Merge a group of images along channels dimensions. \n",
    "    \n",
    "    Count is the number of images to be merged starting from the first one.\n",
    "    If count < len(images), then only the first count images are merge along the\n",
    "    channels dimension. The merged image is put in an array, and the other are\n",
    "    appended to this array as a subarray. \n",
    "    Example: [im1, im2, im2], count=2  --> [ merged(im1,im2) , [im3]] \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, count=-1, axis=-1):\n",
    "        self.axis  = axis\n",
    "        self.count = count\n",
    "    def __call__(self, images):\n",
    "        if isinstance(images, collections.Sequence) or isinstance(images, np.ndarray):\n",
    "            assert all([isinstance(i, np.ndarray) for i in images]), 'only numpy array is supported'\n",
    "            shapes = [list(i.shape) for i in images]\n",
    "            for s in shapes:\n",
    "                s[self.axis] = None\n",
    "            assert all([s==shapes[0] for s in shapes]), 'shapes must be the same except the merge axis'\n",
    "            if(self.count > 0):\n",
    "                merged = np.concatenate(images[0:min(self.count, len(images))], axis=self.axis)\n",
    "                if(self.count < len(images)):\n",
    "                    tab_output = [merged]\n",
    "                    tab_output.append(images[self.count:len(images)])\n",
    "                    return tab_output\n",
    "                else:\n",
    "                    return merged\n",
    "            else:\n",
    "                merged = np.concatenate(images, axis=self.axis)\n",
    "                return merged\n",
    "        else:\n",
    "            raise Exception(\"obj is not a sequence (list, tuple, etc)\")\n",
    "        \n",
    "class Split(object):\n",
    "    \"\"\"Split images into individual images\n",
    "    \"\"\"\n",
    "    def __init__(self, *slices, **kwargs):\n",
    "        assert isinstance(slices, collections.Sequence)\n",
    "        slices_ = []\n",
    "        for s in slices:\n",
    "            if isinstance(s, collections.Sequence):\n",
    "                slices_.append(slice(*s))\n",
    "            else:\n",
    "                slices_.append(s)\n",
    "        assert all([isinstance(s, slice) for s in slices_]), 'slices must be consist of slice instances'\n",
    "        self.slices = slices_\n",
    "        self.axis = kwargs.get('axis', -1)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        ret = []\n",
    "        for s in self.slices:\n",
    "            sl = [slice(None)]*image.ndim\n",
    "            sl[self.axis] = s\n",
    "            ret.append(image[sl])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define numpy ready transform functions\n",
    "Because PIL cannot handle images with more than 4 channels, we need to rewrite all useful transformation using numpy to use above code. Long but worth it !\n",
    "These functions are used to transform our data. \n",
    "We reimplement some functional transforms for a numpy interface instead of a PIL interface. The problem with PIL Interface is that PIL images cannot be used with more than 4 channels.\n",
    "<br> Original source code : \n",
    "https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, functionals\n",
    "Following Pytorch architecture, we will first define some functionals that will later be used by transform classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img, i, j, h, w):\n",
    "    \"\"\"Crop the given Numpy Array.\n",
    "    Preserve number of channels.\n",
    "    Args:\n",
    "        img (Numpy Array): Image to be cropped.\n",
    "        i (int): i in (i,j) i.e coordinates of the upper left corner.\n",
    "        j (int): j in (i,j) i.e coordinates of the upper left corner.\n",
    "        h (int): Height of the cropped image.\n",
    "        w (int): Width of the cropped image.\n",
    "    Returns:\n",
    "        Numpy Array: Cropped array.\n",
    "    \"\"\"\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        raise TypeError('img should be a numpy array. Got {}'.format(type(img)))\n",
    "    return img[j:j+w, i:i+h, :]\n",
    "\n",
    "def center_crop(img, output_size):\n",
    "    \"\"\"Crop the given Numpy Array around its WxH center.\n",
    "       Preserve number of channels.\n",
    "    Args:\n",
    "        img (Numpy Array)    : Image to be cropped.\n",
    "        output_size (integer): Size of the square crop.\n",
    "    Returns:\n",
    "        Numpy Array: Cropped array.\n",
    "    \"\"\"\n",
    "    w, h = img.shape[0], img.shape[1]\n",
    "    th, tw = output_size, output_size\n",
    "    i = int(round((h - th) / 2.))\n",
    "    j = int(round((w - tw) / 2.))\n",
    "    return  crop(img, i, j, th, tw)\n",
    "\n",
    "def resized_crop(img, i, j, h, w, size, interpolation=cv2.INTER_LINEAR):\n",
    "    \"\"\"Crop the given numpy array and resize it to desired size.\n",
    "        Notably used in :class:`RandomResizedCrop`.\n",
    "        Args:\n",
    "            img (Numpy Array) : Image to be cropped.\n",
    "            i (int): i in (i,j) i.e coordinates of the upper left corner\n",
    "            j (int): j in (i,j) i.e coordinates of the upper left corner\n",
    "            h (int): Height of the cropped image.\n",
    "            w (int): Width of the cropped image.\n",
    "            size (sequence or int): Desired output size. Same semantics as ``resize``.\n",
    "            interpolation (int, optional): Desired interpolation. OpenCV is used for this.\n",
    "            Default is ``bilinear`` with ``cv2.INTER_LINEAR``.\n",
    "        Returns:\n",
    "            Numpy Array: Cropped-resized image.\n",
    "    \"\"\"\n",
    "    assert isinstance(img, np.ndarray),'img should be Numpy array'\n",
    "    img = crop(img, i, j, h, w)\n",
    "    img = cv2.resize(img, dsize=size, interpolation=interpolation)\n",
    "    return img\n",
    "\n",
    "def hflip(img):\n",
    "    \"\"\"Horizontally flip the given Numpy Array\n",
    "    Args:\n",
    "        img (Numpy Array): Image to be flipped.\n",
    "    Returns:\n",
    "        Numpy Array:  Horizontally flipped image.\n",
    "    \"\"\"\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        raise TypeError('img should be Numpy array: Got {}'.format(type(img)))\n",
    "    return cv2.flip( img, 0 )\n",
    "\n",
    "def vflip(img):\n",
    "    \"\"\"Vertically flip the given Numpy Array\n",
    "    Args:\n",
    "        img (Numpy Array): Image to be flipped.\n",
    "    Returns:\n",
    "        Numpy Array:  Vertically flipped image.\n",
    "    \"\"\"\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        raise TypeError('img should be Numpy array: Got {}'.format(type(img)))\n",
    "    return cv2.flip( img, 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, transfrom classes\n",
    "Now, we will define the classes the will make use of our functionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given Numpy Array at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Numpy Array): Image to be cropped.\n",
    "        Returns:\n",
    "            Numpy Array: Cropped image.\n",
    "        \"\"\"\n",
    "        return center_crop(img, self.size)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
    "    \n",
    "\n",
    "class RandomResizedCrop(object):\n",
    "    \"\"\"Crop the given Numpy Array Image to random size and aspect ratio.\n",
    "        A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n",
    "        aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n",
    "        is finally resized to given size.\n",
    "        This is popularly used to train the Inception networks.\n",
    "        Args:\n",
    "            size: expected output size of each edge\n",
    "            scale: range of size of the origin size cropped\n",
    "            ratio: range of aspect ratio of the origin aspect ratio cropped\n",
    "            interpolation: Default: cv2.INTER_LINEAR\n",
    "    \"\"\"\n",
    "    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=cv2.INTER_LINEAR):\n",
    "        if isinstance(size, tuple):\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n",
    "            warnings.warn(\"range should be of kind (min, max)\")\n",
    "        self.interpolation = interpolation\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_params(img, scale, ratio):\n",
    "        \"\"\"Get parameters for ``crop`` for a random sized crop.\n",
    "            Args:\n",
    "                img (Numpy array): Image to be cropped.\n",
    "                scale (tuple): range of size of the origin size cropped\n",
    "                ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n",
    "            Returns:\n",
    "                tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n",
    "                    sized crop.\n",
    "        \"\"\"\n",
    "        area = img.shape[0] * img.shape[1]\n",
    "        for attempt in range(10):\n",
    "            target_area = random.uniform(*scale) * area\n",
    "            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n",
    "            aspect_ratio = math.exp(random.uniform(*log_ratio))\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "            print(w,h)\n",
    "            if w <= img.shape[0] and h <= img.shape[1]:\n",
    "                i = random.randint(0, img.shape[1] - h)\n",
    "                j = random.randint(0, img.shape[0] - w)\n",
    "                return i, j, h, w\n",
    "        \n",
    "        # Fallback to central crop\n",
    "        in_ratio = img.shape[0] / img.shape[1]\n",
    "        if (in_ratio < min(ratio)):\n",
    "            w = img.shape[0]\n",
    "            h = int(round(w / min(ratio)))\n",
    "        elif (in_ratio > max(ratio)):\n",
    "            h = img.shape[1]\n",
    "            w = int(round(h * max(ratio)))\n",
    "        else:  # whole image\n",
    "            w = img.shape[0]\n",
    "            h = img.shape[1]\n",
    "        \n",
    "        i = (img.shape[1] - h) // 2\n",
    "        j = (img.shape[0] - w) // 2\n",
    "        return i, j, h, w\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Numpy Array): Image to be cropped and resized.\n",
    "        Returns:\n",
    "            Numpy Array: Randomly cropped and resized image.\n",
    "        \"\"\"\n",
    "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
    "        return resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = str(self.interpolation) # number for OpenCV enum / should be map to names\n",
    "        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n",
    "        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n",
    "        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n",
    "        format_string += ', interpolation={0})'.format(interpolate_str)\n",
    "        return format_string\n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given Numpy Array randomly with a given probability.\n",
    "        Args:\n",
    "            p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                img (Numpy Array): Image to be flipped.\n",
    "            Returns:\n",
    "                Numpy Array: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return hflip(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "    \n",
    "class RandomVerticalFlip(object):\n",
    "    \"\"\"Vertically flip the given Numpy Array randomly with a given probability.\n",
    "        Args:\n",
    "            p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                img (Numpy Array): Image to be flipped.\n",
    "            Returns:\n",
    "                Numpy Array: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return vflip(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our implementation on demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 191\n",
      "(25, 25, 6) (25, 25, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remi1/Jupyter/jupyter-env/lib/python3.5/site-packages/ipykernel_launcher.py:80: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbfb6828da0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load thre images\n",
    "im1 = io.imread(\"demo_img/img_11.png\")\n",
    "im2 = io.imread(\"demo_img/img_13.png\") \n",
    "im3 = io.imread(\"demo_img/img_15.png\") \n",
    "ims = [im1, im2, im3]\n",
    "\n",
    "# Define our transformation : \n",
    "# first merge, \n",
    "# second transform, \n",
    "# third split keeping first 2 images together \n",
    "transform = transforms.Compose([\n",
    "    Merge(3),\n",
    "    RandomResizedCrop(25),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    Split([0,6], [6,9]),\n",
    "])\n",
    "res = transform(ims)\n",
    "print(res[0].shape, res[1].shape)\n",
    "\n",
    "\n",
    "# Split the first two images\n",
    "# that were let together\n",
    "spliter = transforms.Compose([\n",
    "    Split([0,3], [3,6])\n",
    "])\n",
    "vis = spliter(res[0])\n",
    "\n",
    "# Visualize cropping result\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(vis[0])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(vis[1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
